## План исследования

### Тема магистерской работы  
«Интерпретируемость больших языковых моделей: методы анализа и улучшения прозрачности решений с использованием sparse autoencoders»

---

### 1. Введение (1-2 месяца)  
- Проблематика:  
  - Почему интерпретируемость важна? Риски «чёрного ящика» в LLM: предвзятость, ошибки, отсутствие доверия.  
  - Примеры: случаи генерации ложной информации (hallucinations), токсичного контента, культурных стереотипов.  
- Цель исследования:  
  - Разработать/адаптировать методы для анализа внутренних механизмов LLM, включая использование sparse autoencoders для повышения прозрачности их решений.  
- Задачи:  
  1. Классифицировать существующие методы интерпретируемости.  
  2. Сравнить их эффективность на разных типах задач (классификация, генерация).  
  3. Исследовать применение sparse autoencoders для анализа скрытых представлений LLM.  
  4. Предложить улучшения для выбранных методов.  
  5. Оценить влияние интерпретируемости на доверие пользователей.  

---

### 2. Обзор литературы (2-3 месяца)  
- Основные направления интерпретируемости:  
  1. Локальные методы: LIME, SHAP, Integrated Gradients.  
  2. Анализ внимания (Attention Maps): интерпретация слоев трансформера.  
  3. Probing-классификаторы: исследование, какие знания кодируют скрытые представления.  
  4. Sparse Autoencoders: выделение семантически значимых компонент в активациях.  
  5. Контролируемая генерация: методы влияния на выход модели через промпты.  
- Ключевые работы:  
  - «Attention is Not Explanation» (Jain & Wallace, 2019).  
  - «Transformer Interpretability Beyond Attention Visualization» (Chefer et al., 2021).  
  - Исследования по sparse autoencoders (например, работы Anthropic, EleutherAI).  

---

### 3. Методология (3-4 месяца)  
#### Этап 1. Выбор моделей и данных  
- Модели:  
  - BERT/RoBERTa (для классификации), GPT-3/LLAMA (для генерации).  
  - Возможно использование открытых моделей (например, Mistral, Falcon).  
- Датасеты:  
  - Текстовые данные с аннотациями (например, IMDB для классификации, Squad для问答).  
  - Специализированные данные для анализа смещений (например, StereoSet).  

#### Этап 2. Эксперименты с существующими методами  
1. Локальная интерпретируемость:  
   - Применение SHAP/LIME для анализа решений BERT в задачах классификации.  
   - Сравнение важности токенов с картами внимания.  
2. Анализ скрытых представлений:  
   - Обучение probing-классификаторов для предсказания грамматических/семантических свойств.  
3. Sparse Autoencoders:  
   - Обучение sparse autoencoders на активациях LLM для выделения семантически значимых компонент.  
   - Визуализация и интерпретация выделенных компонент.  

#### Этап 3. Разработка улучшений  
- Гипотезы:  
  - Добавление регуляризации для повышения интерпретируемости внимания.  
  - Создание гибридных методов (например, комбинация SHAP и анализа внимания).  
  - Использование sparse autoencoders для улучшения интерпретируемости скрытых представлений.  

---

### 4. Эксперименты и результаты (4-5 месяцев)  
- Эксперимент 1: Сравнение методов интерпретируемости на задачах классификации.  
  - Метрики:  
    - Consistency (насколько стабильны объяснения для похожих входов).  
    - Faithfulness (насколько объяснения отражают реальные причины решений модели).  
  - Пример вывода: «Метод SHAP показывает большую согласованность, чем attention maps, но требует больше вычислительных ресурсов».  

- Эксперимент 2: Анализ смещений в генеративных моделях.  
  - Подход:  
    - Генерация текстов по промптам с гендерными/культурными ключами.  
    - Использование датасета CrowS-Pairs для оценки стереотипов.  
  - Пример вывода: «LLAMA 2 генерирует на 15% меньше гендерных стереотипов, чем GPT-2».